{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bd5e2af",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_________________________________________________\n",
      "\n",
      "20250807 02:50:56 train.py main() running.\n",
      "True\n",
      "checkpoint: check_default\n",
      "data_path: ../data\n",
      "depth: 4\n",
      "epochs: 20\n",
      "lr: 0.001\n",
      "lr_factor: 0.1\n",
      "lr_schedule: [100, 150]\n",
      "model: maze_ut\n",
      "model_path: None\n",
      "shuffle: True\n",
      "optimizer: adam\n",
      "output: output_default\n",
      "quick_test: False\n",
      "save_json: False\n",
      "save_period: 20\n",
      "test_batch_size: 500\n",
      "test_iterations: None\n",
      "test_mode: default\n",
      "train_batch_size: 128\n",
      "train_log: train_log.txt\n",
      "train_mode: default\n",
      "val_period: 20\n",
      "warmup_period: 5\n",
      "width: 4\n",
      "logging done in output_default.\n",
      "Files already downloaded and verified\n",
      "Loading mazes of size 9 x 9.\n",
      "Files already downloaded and verified\n",
      "Loading mazes of size 15 x 15.\n",
      "This maze_ut has 1.410 million parameters.\n",
      "Training will start at epoch 0.\n",
      "==> Starting training for 20 epochs...\n",
      "Warning: best_model_state와 model_path 모두 없습니다. 랜덤 파라미터로 테스트합니다.\n",
      "DEBUG inputs.shape:  torch.Size([128, 3, 24, 24])\n",
      "DEBUG targets.shape:  torch.Size([128, 24, 24])\n",
      "DEBUG unique labels:  tensor([0, 1])\n",
      "20250807 02:52:00 Training loss at epoch 0: 0.6863476991653442                  \n",
      "20250807 02:52:00 Training accuracy at epoch 0: 0.0\n",
      "20250807 02:52:00 Training accuracy at best epoch 0:0\n",
      "Early Stopping Check Point : 1\n",
      "20250807 02:52:57 Training loss at epoch 1: 0.6807529232440851                  \n",
      "20250807 02:52:57 Training accuracy at epoch 1: 0.0\n",
      "20250807 02:52:57 Training accuracy at best epoch 0:0\n",
      "Early Stopping Check Point : 2\n",
      "20250807 02:53:55 Training loss at epoch 2: 0.6800976331417378                  \n",
      "20250807 02:53:55 Training accuracy at epoch 2: 0.0\n",
      "20250807 02:53:55 Training accuracy at best epoch 0:0\n",
      "Early Stopping Check Point : 3\n",
      "20250807 02:54:52 Training loss at epoch 3: 0.6800793857146532                  \n",
      "20250807 02:54:52 Training accuracy at epoch 3: 0.0\n",
      "20250807 02:54:52 Training accuracy at best epoch 0:0\n",
      "Early Stopping Check Point : 4\n",
      "20250807 02:55:50 Training loss at epoch 4: 0.6802528726748931                  \n",
      "20250807 02:55:50 Training accuracy at epoch 4: 0.0\n",
      "20250807 02:55:50 Training accuracy at best epoch 0:0\n",
      "Early Stopping Check Point : 5\n",
      "20250807 02:56:47 Training loss at epoch 5: 0.6799050060602335                  \n",
      "20250807 02:56:47 Training accuracy at epoch 5: 0.0\n",
      "20250807 02:56:47 Training accuracy at best epoch 0:0\n",
      "Early Stopping Check Point : 6\n",
      "20250807 02:57:44 Training loss at epoch 6: 0.6796781980074369                  \n",
      "20250807 02:57:44 Training accuracy at epoch 6: 0.0\n",
      "20250807 02:57:44 Training accuracy at best epoch 0:0\n",
      "Early Stopping Check Point : 7\n",
      "20250807 02:58:41 Training loss at epoch 7: 0.6796284087193318                  \n",
      "20250807 02:58:41 Training accuracy at epoch 7: 0.0\n",
      "20250807 02:58:41 Training accuracy at best epoch 0:0\n",
      "Early Stopping Check Point : 8\n",
      "20250807 02:59:38 Training loss at epoch 8: 0.6794132004945707                  \n",
      "20250807 02:59:38 Training accuracy at epoch 8: 0.0\n",
      "20250807 02:59:38 Training accuracy at best epoch 0:0\n",
      "Early Stopping Check Point : 9\n",
      "20250807 03:00:35 Training loss at epoch 9: 0.6793390171650129                  \n",
      "20250807 03:00:35 Training accuracy at epoch 9: 0.0\n",
      "20250807 03:00:35 Training accuracy at best epoch 0:0\n",
      "Early Stopping Check Point : 10\n",
      "20250807 03:01:32 Training loss at epoch 10: 0.679208663946543                  \n",
      "20250807 03:01:32 Training accuracy at epoch 10: 0.0\n",
      "20250807 03:01:32 Training accuracy at best epoch 0:0\n",
      "Early Stopping Check Point : 11\n",
      "20250807 03:02:29 Training loss at epoch 11: 0.6790125357799041                 \n",
      "20250807 03:02:29 Training accuracy at epoch 11: 0.0\n",
      "20250807 03:02:29 Training accuracy at best epoch 0:0\n",
      "Early Stopping Check Point : 12\n",
      "20250807 03:03:27 Training loss at epoch 12: 0.6789996947997655                 \n",
      "20250807 03:03:27 Training accuracy at epoch 12: 0.0\n",
      "20250807 03:03:27 Training accuracy at best epoch 0:0\n",
      "Early Stopping Check Point : 13\n",
      "20250807 03:04:24 Training loss at epoch 13: 0.6788830182491205                 \n",
      "20250807 03:04:24 Training accuracy at epoch 13: 0.0\n",
      "20250807 03:04:24 Training accuracy at best epoch 0:0\n",
      "Early Stopping Check Point : 14\n",
      "20250807 03:05:21 Training loss at epoch 14: 0.6789189237814683                 \n",
      "20250807 03:05:21 Training accuracy at epoch 14: 0.0\n",
      "20250807 03:05:21 Training accuracy at best epoch 0:0\n",
      "Early Stopping Check Point : 15\n",
      "20250807 03:06:18 Training loss at epoch 15: 0.678874536202504                  \n",
      "20250807 03:06:18 Training accuracy at epoch 15: 0.0\n",
      "20250807 03:06:18 Training accuracy at best epoch 0:0\n",
      "Early Stopping Check Point : 16\n",
      "20250807 03:07:15 Training loss at epoch 16: 0.6788532723218966                 \n",
      "20250807 03:07:15 Training accuracy at epoch 16: 0.0\n",
      "20250807 03:07:15 Training accuracy at best epoch 0:0\n",
      "Early Stopping Check Point : 17\n",
      "20250807 03:08:12 Training loss at epoch 17: 0.6788767544122842                 \n",
      "20250807 03:08:12 Training accuracy at epoch 17: 0.0\n",
      "20250807 03:08:12 Training accuracy at best epoch 0:0\n",
      "Early Stopping Check Point : 18\n",
      "20250807 03:09:09 Training loss at epoch 18: 0.6788525345997932                 \n",
      "20250807 03:09:09 Training accuracy at epoch 18: 0.0\n",
      "20250807 03:09:09 Training accuracy at best epoch 0:0\n",
      "Early Stopping Check Point : 19\n",
      "20250807 03:10:06 Training loss at epoch 19: 0.6788145841696324                 \n",
      "20250807 03:10:06 Training accuracy at epoch 19: 0.0\n",
      "20250807 03:10:06 Training accuracy at best epoch 0:0\n",
      "Early Stopping Check Point : 20\n",
      "Traceback (most recent call last):                                              \n",
      "  File \"/shared/home/lrlab/LoopWorks/Team3/hb/Loopedmodel_research/Capstone-2025-1-team3-main/UT/train.py\", line 271, in <module>\n",
      "    main()\n",
      "  File \"/shared/home/lrlab/LoopWorks/Team3/hb/Loopedmodel_research/Capstone-2025-1-team3-main/UT/train.py\", line 196, in main\n",
      "    test_acc = test(net, testloader, args.test_mode, device)\n",
      "  File \"/shared/home/lrlab/LoopWorks/Team3/hb/Loopedmodel_research/Capstone-2025-1-team3-main/UT/utils.py\", line 115, in test\n",
      "    accuracy = eval(f\"test_{mode}\")(net, testloader, device)\n",
      "  File \"/shared/home/lrlab/LoopWorks/Team3/hb/Loopedmodel_research/Capstone-2025-1-team3-main/UT/utils.py\", line 131, in test_default\n",
      "    outputs = net(inputs)\n",
      "  File \"/shared/home/lrlab/LoopWorks/Team3/hb/Loopedmodel_research/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/shared/home/lrlab/LoopWorks/Team3/hb/Loopedmodel_research/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/shared/home/lrlab/LoopWorks/Team3/hb/Loopedmodel_research/Capstone-2025-1-team3-main/UT/models/maze_ut.py\", line 95, in forward\n",
      "    x = x + self.pos_embed[:, :H * W, :]\n",
      "RuntimeError: The size of tensor a (1296) must match the size of tensor b (1024) at non-singleton dimension 1\n"
     ]
    }
   ],
   "source": [
    "!MPLBACKEND=agg python train.py \\\n",
    "  --model maze_ut \\\n",
    "  --depth 4 \\\n",
    "  --width 4 \\\n",
    "  --epochs 20 \\\n",
    "  --train_batch_size 128 \\\n",
    "  --lr 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a88422b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
